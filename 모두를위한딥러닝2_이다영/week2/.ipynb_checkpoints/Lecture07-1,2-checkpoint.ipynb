{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application & Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learing rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "- learnign rate와 기울기(Gradient)의 상관관계를 통해 최적화해감\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(hypothesis, lables):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value=loss_fn(hypothesis, labels)\n",
    "    return tape_gradient(loss_value, [W,b])\n",
    "#optimizer= tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good and Bad\n",
    "- learnign rate를 잘 설정해야 최적화가 잘 됨\n",
    "    - 너무 크면 overshooting\n",
    "    - 너무 작으면 시간이 오래걸림\n",
    "    - 가장 잘 쓰는 Learining Rate는 **0.01**\n",
    "    - Adam Optimizer을 위한 Learing Rate 는 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annealing the learing rate\n",
    "- 학습을 하는 과정에서 점점 cost값이 떨어지고, 더이상 학습이 되지 않는 현상을 해결 할 때 learning rate를 줄이는 **Learning Rate Decay**기법\n",
    "    - learging rate를 조졀하며 최적화 학습시킴\n",
    "- tensorflow 코드에서 원하는 learing rate값 조절 할 수 있음\n",
    "    - tf.train.exponential_decay\n",
    "    - tf.trian.natural_exp_decay\n",
    "    - ...등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "데이터 아래에 밀집되어있을 때\n",
    "\n",
    "- Standardization : 가장 많이 쓰는 표준화 기법\n",
    "    - (x-평균)/표준편차\n",
    "- Nomalization : 정규화(0~1)\n",
    "    - (x-x_min)/(x_max-x_min)\n",
    "    \n",
    "- 둘 다 numpy 에서 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Data\n",
    "쓸모없는 데이터를 없애는 것이 중요\n",
    "전처리를 통해 데이터를 없앰\n",
    "\n",
    "- 자연어처리\n",
    "    - 조동사, 조사, 특수문자는 썩 필요한 문자가 아님\n",
    "    - 의미없는 data 없애고 학습에 유용한 data만 남김"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "- 과하게 합쳐져있다.\n",
    "- 학습이 반복될수록 모델이 가설에 가까워짐.\n",
    "    - test 과정에서는 정확도가 떨어지는 경우가 잦음 : 비이상적\n",
    "    - 학습이 덜 된 상태(underfit)\n",
    "    - 학습이 너무 많이 되면 데이터 모양에 맞게 꼬불꼬불(overfit) : variance가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a features\n",
    "feature를 어떻게 정하느냐에 따라 overfitting 해결 가능\n",
    "- **Get more training data(학습 데이터를 더 많이 넣는다)** : data를 많이 넣음으로서희석시킴\n",
    "- **feature 수 줄임** : 2차원 속성->1차원으로 재배치함으로 의미를 더 분명히 한다.\n",
    "    - 가장 많이 사용하는 방법 PCA\n",
    "- **faature 수 늘림** : 모델 너무 심플하면 모델을 구체화 할 필요가 있음. underfitting인 경우에 해결 가능. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization (Add term to loss)\n",
    "정규화. 특정 값을 추가해서 정규화 가능\n",
    "- loss값에 term을 줌으로서 해결 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
