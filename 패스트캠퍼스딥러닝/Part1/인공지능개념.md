# 딥러닝의 구조
![Dlimg](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/dl1.PNG?raw=true)
- data가 model에 들어가 예측을 한다.
- 정답이 label. lable과 예측값 loss와 비교해 틀린지 확인을 하고 최적화(Optm)
- 원하는 만큼 학습을 하고 나면 결과(result)가 나옴.

===
### Data
- 학습시키기 위한 데이터. 이 데이터가 모델에 들어간다.

### Model
- 학습을 시키는 것.
- data가 들어왔을 때 특징을 찾아내 class를 분류해줌

### Prediction/Logit
- Model을 통해 나온 값.
- 각 Class별로 **예측**한 값.

### Loss/Cost
- 예측값과 정답이 얼마나 가까운 지 확인해서 얼마나 틀렸는지를 계산한 값.
 - 이를 계산하기 위해 Loss/Cost Function을 사용함

### Optimization
- Loss값을 최소화하기 위해 Optimization(최적화) 하여 인공지능의 성능을 높힌다.

### Result
- 마지막에 예측값 중 argmax를 통해 가장 높은 값.

# 딥러닝 용어
## model
![Alt text](https://www.researchgate.net/profile/Margaret_Lech/publication/314200586/figure/fig1/AS:614170164727826@1523440986689/Structures-of-AlexNet-top-and-structure-of-our-system-with-part-of-AlexNet-applied-as.png)
- 딥러닝이 학습시키고자 하는 대상.
- 모델 안에서 여러 특징을 뽑아내고 예측을 한다.
- 대표적으로 **CNN**을 예로 들 수 있다.
- LeNet, AlexNet, VGG, ResNet 등 다양하게 설계된 모델이 있다.

## Layer
![Layer](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Layer.PNG?raw=true)
- 예전에는 단순하게 n층만 쌓았다.
- 왼쪽의 그림과 같이 여러 층을 쌓았다 해서 "딥러닝" 이라 한다.
    - 이 Layer을 어떻게 쌓았냐, 이 Layer가 무엇이냐가 딥러닝의 핵심
    - Hidden layer : convolution, pooling, Activation Funtion, ...
    - 어떻게 쌓냐? : model
- 오른쪽의 VGG : Model
    - Layer을 쌓은 방식.
    - 원소 하나하하나가 Layer인데 이 Layer 개수를 '층'으로 표현하기도 함.
    - 예전에는 깊이 쌓는 것이 고민이었다면, 요즘은 이 Layer을 효율적으로 쌓는 것이 주된 문제.
    - 깊이 쌓을수록 featuer를 디테일하게 할 수 있음

## Convolution
![Convolution](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Convolution.PNG?raw=true)
- CNN의 C가 Convolution(합성곱)
- 이미지와 합성을 해서 특징을 뽑아내는 것.
- 왼쪽의 이미지가 가운데의 필터를 받아 하나하나 곱해주면 오른쪽의 이미지가 된다.
- 우리가 카메라에서 필터를 사용 할 때 이런 Convolution을 사용함.

## weight/Filter/Kernel/Variable/Bias
![weight](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/weight.PNG?raw=true)
> 자세하게 들어가면 다 다른 말 이지만 대략적인 개념은 같으므로 함께 설명
- 학습하려는 대상.
- 더 좋은 특징을 잡는 Convolution을 잡기 위해 학습을 시킨다.
 - convolution안에서 weight를 학습시킨다.
 - y= Wx+B
  - y는 output x는 input W는 weight B는 Bias

## Pooling Layer
![PoolingLayer](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/PoolingLayer.PNG?raw=true)
- feature를 뽑으면 Pooling Layer로 압축.
- Max Pooling : 가장 큰 특징(높은 수치)들을 뽑아서 이미지를 **압축**한다.


## Optimization
![Optimization](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Optimization.PNG?raw=true)
- 어떻게 하면 Loss를 최소로 할 수 있을지.

## Activation Fuction
![ActFunc](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/ActivationFunction.PNG?raw=true)
- 특징을 뽑고 불필요한 값 제거함.
- 처음에는 ReLU에 익숙해지기(음수 값 제거하는 것)

## SoftMax
![Softmax](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/softmax.PNG?raw=true)
- 마지막에 컴퓨터가 예측해서 나온 class를 확률로 나타내주는 것.


## Loss/Cost Function
![LossFunc](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/LossFunction.PNG?raw=true)
- 컴퓨터가 얼마나 틀렸는지를 계산하는 방식
 - 그렇게 해서 나온 값(틀린 값)이 Loss/Cost

## Learning Rate
![LearningRate](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/LearningRate.PNG?raw=true)
- hyper parameter 중 하나
 - 컴퓨터가 직접 설정할 수 없는 부분을 사람이 학습시킬 때 조절해주는 것.
- Optimization에서 너무 적게 내려가면(왼쪽 그림) 시간이 너무 오래 걸림
- 반대로 너무 많이 내려가면(오른쪽 그림) 정확도가 떨어짐
- 이런 걸 조절 해 주는 것이 Learning Rate

## Batch Size
![Batchsize](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/BatchSize.PNG?raw=true)
- model에 모든 data를 한번에 넣을 수 없다.
- 몇 장을 나눠서 조금씩 넣어주는데, 여기서 '몇 장'이 Batch Size 이다.

## Epoch[에폭]/Step
![Epoch](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Epoch.PNG?raw=true)
- Data를 받고 난 뒤 여러번 반복해서 봐야 함.
    - 반복하는 횟수가 Epoch.

## Train/Validation/Test
![Train](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Train.PNG?raw=true)
- Data를 받으면 Train Set과 Test Set으로 나눔.
- 인공지능을 학습시킬 때는 Train Set으로 학습시키고, 평가할 때 는 Test Set으로 평가.


## Label/Ground Truth
![label](https://github.com/youngDaLee/Deep-learning-Study/blob/master/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4%EB%94%A5%EB%9F%AC%EB%8B%9D/Part1/img/Label.PNG?raw=true)
- Data Set의 정답을 Label 혹은 Ground Truth라고 함.


====
# CNN 모델 구조
## 